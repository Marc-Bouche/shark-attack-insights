{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def3b206-030f-419c-a495-4d7805487d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "!pip install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc254da6-d774-4a75-8f66-074529bfd850",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hypothesis 1\n",
    "# Unprovoked shark attack incidents have a lower fatality rate than provoked incidents.\n",
    "\n",
    "# Loading Data\n",
    "shark_data = pd.read_excel('https://www.sharkattackfile.net/spreadsheets/GSAF5.xls')\n",
    "shark_data.head()\n",
    "s_d = shark_data.iloc[:,:14]\n",
    "\n",
    "# cleaning\n",
    "\n",
    "s_d = s_d.dropna(how='all')\n",
    "s_d = s_d.drop_duplicates()\n",
    "\n",
    "s_d[\"Injury\"].value_counts()\n",
    "shark_data[\"Injury\"].nunique()\n",
    "\n",
    "s_d[\"Type\"].replace(\" Provoked\",\"Provoked\",inplace=True)\n",
    "s_d[\"Injury\"] = s_d[\"Injury\"].apply(lambda x: x.lower() if type(x) == str else x) # making sure spelling is the same\n",
    "\n",
    "## Creating filters to filter for \"fatal\" and \"non-fatal\"\n",
    "\n",
    "condition1 = s_d[\"Injury\"].str.contains(\"fatal\") == True\n",
    "condition11 = s_d[\"Injury\"].str.contains(\"non-fatal\") != True\n",
    "condition12 = s_d[\"Injury\"].str.contains(\"not confirmed\") != True\n",
    "condition13 = s_d[\"Injury\"].str.contains(\"unconfirmed\") != True\n",
    "\n",
    "condition2 =  s_d[\"Injury\"].str.contains(\"fatal\") != True\n",
    "condition21 =  s_d[\"Injury\"].str.contains(\"unknown\") != True\n",
    "\n",
    "condition3 = s_d[\"Injury\"].str.contains(\"non-fatal\") == True\n",
    "\n",
    "### Creating filtered data frames\n",
    "\n",
    "filter_fatal = s_d[condition1 & condition11 & condition12 & condition13]\n",
    "filter_non_fatal = s_d[condition2 & condition21]\n",
    "filter_non_fatal2 = s_d[condition3]  \n",
    "\n",
    "s_d_unprovoked = s_d[s_d[\"Type\"] == \"Unprovoked\"]\n",
    "s_d_provoked = s_d[s_d[\"Type\"] == \"Provoked\"]\n",
    "\n",
    "#### Replace different values with coherent single values\n",
    "\n",
    "filter_fatal[\"Injury\"] = filter_fatal[\"Injury\"].apply(lambda x: \"fatal\")\n",
    "filter_non_fatal[\"Injury\"] = filter_non_fatal[\"Injury\"].apply(lambda x: \"non-fatal\")\n",
    "filter_non_fatal2[\"Injury\"] = filter_non_fatal2[\"Injury\"].apply(lambda x: \"non-fatal\")\n",
    "filter_non_fatal_concat = pd.concat([filter_non_fatal2, filter_non_fatal])\n",
    "\n",
    "##### Creating filtered data frames\n",
    "\n",
    "s_d_unprovoked_fatal = filter_fatal[filter_fatal[\"Type\"] == \"Unprovoked\"]\n",
    "s_d_unprovoked_non_fatal = filter_non_fatal_concat[filter_non_fatal_concat[\"Type\"] == \"Unprovoked\"]\n",
    "\n",
    "s_d_provoked_fatal = filter_fatal[filter_fatal[\"Type\"] == \"Provoked\"]\n",
    "s_d_provoked_non_fatal = filter_non_fatal_concat[filter_non_fatal_concat[\"Type\"] == \"Provoked\"]\n",
    "\n",
    "# Testing\n",
    "## Creating a final summarized data frame from the separate filterd dfs\n",
    "\n",
    "df_h1 = pd.concat([s_d_unprovoked_fatal, s_d_unprovoked_non_fatal, s_d_provoked_non_fatal, s_d_provoked_fatal])\n",
    "df_h1.groupby([\"Injury\",\"Type\"])[\"Injury\"].count()\n",
    "df_h1[\"count\"] = 1\n",
    "df_h1[\"count\"].value_counts()\n",
    "\n",
    "# Result\n",
    "\n",
    "df_h1.pivot_table(index = \"Injury\", columns = \"Type\", values = \"count\", aggfunc = \"sum\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2323352d-395c-4a21-8360-2bdd02c33aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis 2 \n",
    "# Great White Sharks are most likely to attack in the USA. \n",
    "\n",
    "# Loading Data\n",
    "import pandas as pd\n",
    "shark_data =pd.read_excel('https://www.sharkattackfile.net/spreadsheets/GSAF5.xls')\n",
    "shark_data.head()\n",
    "# Cleaning: Species\n",
    "shark_data.rename(columns={\"Species \": \"Species\"}, inplace=True) #rename species column\n",
    "#shark_data[\"Species\"].drop(shark_data[(shark_data == 0).any(axis=1)].index, inplace=True) # removes 0 value\n",
    "#shark_data.dropna(subset=['Species'], inplace=True) #removes NaN\n",
    "shark_data[\"Species\"] = shark_data[\"Species\"].apply(lambda x: \"Tiger Shark\" if \"tiger\" in str(x).lower() else x)\n",
    "shark_data[\"Species\"] = shark_data[\"Species\"].apply(lambda x: \"Bull Shark\" if \"bull\" in str(x).lower() else x)\n",
    "shark_data[\"Species\"] = shark_data[\"Species\"].apply(lambda x: \"Blue Pointer\" if \"blue\" in str(x).lower() else x)\n",
    "shark_data[\"Species\"] = shark_data[\"Species\"].apply(lambda x: \"Great White Shark\" if \"white\" in str(x).lower() else x)\n",
    "shark_data[\"Species\"] = shark_data[\"Species\"].apply(lambda x: \"Hammerhead Shark\" if \"hammer\" in str(x).lower() else x)\n",
    "shark_data[\"Species\"] = shark_data[\"Species\"].apply(lambda x: \"Catshark\" if \"cat\" in str(x).lower() else x)\n",
    "shark_data[\"Species\"] = shark_data[\"Species\"].apply(lambda x: \"Hammerhead Shark\" if \"hammer\" in str(x).lower() else x)\n",
    "shark_data[\"Species\"] = shark_data[\"Species\"].apply(lambda x: \"Brown Shark\" if \"brown\" in str(x).lower() else x)\n",
    "shark_data[\"Species\"] = shark_data[\"Species\"].apply(lambda x: \"Blacktip\" if \"black\" in str(x).lower() else x)\n",
    "shark_data[\"Species\"] = shark_data[\"Species\"].apply(lambda x: \"Invalid\" if \"NaN\" in str(x).lower() else x)\n",
    "shark_data[\"Species\"] = shark_data[\"Species\"].apply(lambda x: \"Invalid\" if \"questionable\" in str(x).lower() else x)\n",
    "shark_data[\"Species\"] = shark_data[\"Species\"].apply(lambda x: \"Invalid\" if \"involvement\" in str(x).lower() else x)\n",
    "shark_data[\"Species\"] = shark_data[\"Species\"].apply(lambda x: \"Invalid\" if \"'\" in str(x).lower() else x)\n",
    "shark_data[\"Species\"] = shark_data[\"Species\"].apply(lambda x: \"Invalid\" if \"''\" in str(x).lower() else x)\n",
    "shark_data[\"Species\"] = shark_data[\"Species\"].apply(lambda x: \"Invalid\" if \"[]\" in str(x).lower() else x)\n",
    "# Cleaning: Country\n",
    "def clean_country(country):\n",
    "    # Handle NaN values\n",
    "    if pd.isna(country):\n",
    "        return 'UNSPECIFIED COUNTRY'\n",
    "    # Remove question marks and strip whitespace\n",
    "    country = country.replace('?', '').strip()\n",
    "    # Handle cases where multiple countries are listed\n",
    "    country = country.replace('IRAN / IRAQ', 'IRAN') \\\n",
    "                     .replace('SOLOMON ISLANDS / VANUATU', 'SOLOMON ISLANDS') \\\n",
    "                     .replace('EQUATORIAL GUINEA / CAMEROON', 'CAMEROON') \\\n",
    "                     .replace('CEYLON (SRI LANKA)', 'SRI LANKA') \\\n",
    "                     .replace('EGYPT / ISRAEL', 'EGYPT') \\\n",
    "                     .replace('ITALY / CROATIA', 'ITALY') \\\n",
    "                     .replace('BETWEEN PORTUGAL & INDIA', 'UNSPECIFIED COUNTRY') \\\n",
    "                     .replace('DIEGO GARCIA', 'UNSPECIFIED COUNTRY')\n",
    "    # Replace \"/\" with \"and\" in specific countries\n",
    "    country = country.replace('ANDAMAN / NICOBAR ISLANDS', 'ANDAMAN AND NICOBAR ISLANDS') \\\n",
    "                     .replace('ST KITTS / NEVIS', 'ST KITTS AND NEVIS')\n",
    "    # Mapping replacements for specific island entries\n",
    "    replacements = {\n",
    "        'UNITED ARAB EMIRATES (UAE)': 'UNITED ARAB EMIRATES',\n",
    "        'NEW GUINEA / PAPUA NEW GUINEA': 'PAPUA NEW GUINEA',\n",
    "        'SOLOMON ISLANDS / VANUATU': 'SOLOMON ISLANDS',\n",
    "        'MALDIVE ISLANDS': 'MALDIVES',\n",
    "        'ST. MAARTIN': 'ST. MARTIN',  # Correct spelling\n",
    "        'KOREA': 'SOUTH KOREA'        # Replace KOREA with SOUTH KOREA\n",
    "    }\n",
    "    # Apply replacements\n",
    "    for key, value in replacements.items():\n",
    "        if country == key:\n",
    "            country = value\n",
    "    # Assign 'UNSPECIFIED COUNTRY' to values containing sea/ocean-related terms\n",
    "    sea_terms = ['sea', 'SEA', 'OCEAN', 'ocean', 'Ocean', 'Sea', 'BAY OF BENGAL', 'AFRICA']\n",
    "    for term in sea_terms:\n",
    "        if term in country:\n",
    "            return 'UNSPECIFIED COUNTRY'\n",
    "    # Remove invalid countries or regions\n",
    "    invalid_countries = ['Diego Garcia', 'GULF OF ADEN', 'THE BALKANS', 'BRITISH ISLES', 'PERSIAN GULF', 'JOHNSTON ISLAND',\n",
    "                         'JAVA', 'ROTAN', 'SAN DOMINGO', 'ST. MARTIN', 'NEVIS', 'GRAND CAYMAN', 'NETHERLANDS ANTILLES',\n",
    "                         'NORTHERN MARIANA ISLANDS', 'ASIA']\n",
    "    if country in invalid_countries:\n",
    "        return 'UNSPECIFIED COUNTRY'\n",
    "    # Convert all to uppercase\n",
    "    country = country.upper()\n",
    "# Group by 'Species' and 'Country', and count the occurrences\n",
    "shark_attacks_count = shark_data.groupby(['Species', 'Country']).size().reset_index(name='Count')\n",
    "# Sort the DataFrame by the Count in descending order\n",
    "shark_attacks_count = shark_attacks_count.sort_values(by='Count', ascending=False)\n",
    "shark_attacks_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf189f1b-4c50-4291-b541-fd215518fc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Number of Attacks\n",
      "Sex                       \n",
      "Female                 744\n",
      "Invalid                580\n",
      "Male                  5474\n"
     ]
    }
   ],
   "source": [
    "## Hypothesis 4 \n",
    "## Males have higher chances than females of being attacked by a shark.  \n",
    "\n",
    "# Loading Data\n",
    "import pandas as pd\n",
    "shark_data =pd.read_excel('https://www.sharkattackfile.net/spreadsheets/GSAF5.xls')\n",
    "shark_data.head()\n",
    "\n",
    "# Cleaning: Sex\n",
    "shark_data[\"Sex\"] = shark_data[\"Sex\"].apply(lambda x: \"Male\" if \"m\" in str(x).lower() else x)\n",
    "shark_data[\"Sex\"] = shark_data[\"Sex\"].apply(lambda x: \"Female\" if \"f\" in str(x).lower() else x)\n",
    "shark_data[\"Sex\"] = shark_data[\"Sex\"].apply(lambda x: \"Invalid\" if \"nan\" in str(x).lower() else x)\n",
    "shark_data[\"Sex\"] = shark_data[\"Sex\"].apply(lambda x: \"Invalid\" if \"n\" in str(x).lower() else x)\n",
    "shark_data[\"Sex\"] = shark_data[\"Sex\"].apply(lambda x: \"Invalid\" if \"lli\" in str(x).lower() else x)\n",
    "shark_data[\"Sex\"] = shark_data[\"Sex\"].apply(lambda x: \"Invalid\" if \"M x 2\" in str(x).lower() else x)\n",
    "shark_data[\"Sex\"] = shark_data[\"Sex\"].apply(lambda x: \"Invalid\" if \".\" in str(x).lower() else x)\n",
    "\n",
    "# Create a pivot table with 'Sex ' as the index and count the number of shark attacks for each category\n",
    "pivot_table = pd.pivot_table(shark_data,index='Sex',values='Case Number', aggfunc='count')\n",
    "pivot_table.columns = ['Number of Attacks']\n",
    "print(pivot_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
